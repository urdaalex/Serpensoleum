import sys
import os
import cPickle as pickle
import simplejson
from gensim.models import Word2Vec as w2v
from gensim.models import Doc2Vec as d2v
import logging
import nltk.data

'''
NOTE:
    The input data has to be parsed using the html parser AND processed
    by the preprocessor
'''

# Changed the way errors generated by word2vec are reported
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def isValid(input_args):
    '''
    Given the list of input arguments, this function will return a boolean
    indicating whether or not the inputs are valid
    '''
    proper_usage = "Input Error: One or more of the following is causing an issue\n" +\
                "\t 1) Too few or too many inputs \n" +\
                "\t 2) The input directory doesn't exist\n" +\
                "\t 3) The output directory already exists\n" +\
                "Correct usage: \n" +\
                "\t clustering_approach.py 'input_dir_name' 'output_dir_name'"

    if len(input_args) != 2 or os.path.exists(input_args[1]) \
                or not os.path.exists(input_args[0]):
        print proper_usage
        return False

    return True

def getSentences(JSON_files):
    '''
    Given a list of JSON files where each JSON file has a dictionary
    'paragraphs' which is a list of the paragraphs in the article
    represented by that JSON file, this function returns a list
    of all the sentences (each sentence will bea list of all the
    words/characters in it) in all the paragraphs of all the JSON files
    '''
    all_sentences = []
    for json in JSON_files:
        paragraphs = json['paragraphs']
        for paragraph in paragraphs:
            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
            sentence = ''.join(tokenizer.tokenize(paragraph))
            all_sentences.append([i for i in sentence.split(' ')])
    return all_sentences

def getDocuments(JSON_files):
    '''
    Given a list of JSON files, where each JSON file has a dictionary 'paragraphs'
    which is a list of the paragraphs in the article represented by that JSON file,
    this function returns a list of documents (each article is a document) &
    its label (list of document, label tuples), where a document representation
    of an article is the concatenation of the paragraphs it contains
    '''
    all_documents = []
    for json in JSON_files:
        paragraphs = json['paragraphs']
        label = json['actual-search-type']
        document = ''
        for paragraph in paragraphs[:-1]:
            document += paragraph + "\n\n"
        document += paragraphs[-1]
        all_documents.append((document, label))
    return all_documents

def main(argv):
    '''
    Given the array of arguments to the program, the main method will ensure
    that the inputs are valid, if they are, the JSON files in the input
    directory will be loaded, and the clustering approach will be applied
    on the data in the input directory. The model will then be saved
    into a pickle file specified by the input arguments
    '''
    # Check that the input arguments are valid
    if not isValid(argv):
        sys.exit(1)

    # Load a list of the JSON files in the input dir
    JSON_files = []
    for filename in os.listdir(argv[0]):
        with open(os.path.join(argv[0], filename), 'r') as json_file:
            JSON_files.append(simplejson.load(json_file))

    # Get all the documents in the JSON files
    documents_and_labels = getDocuments(JSON_files)


if __name__ == "__main__":
    main(sys.argv[1:])
